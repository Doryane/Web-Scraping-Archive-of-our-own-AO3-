{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSOmVaH4ILQySrW1xEP35n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doryane/Web-Scrapping-Archive-of-our-own-AO3-/blob/main/Web_Scrapping_AO3_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "#main()"
      ],
      "metadata": {
        "id": "YZgjylefEeHj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scripping pour AO3 based on following code :\n",
        "https://github.com/kenalba/ao3-scraper/blob/main/scraper.py"
      ],
      "metadata": {
        "id": "ijZ5GY4itGU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# What this code do ?\n",
        "\n",
        "With this code you can append the following information from AO3 and put it into a data frame from a certain page to another\n",
        "\n",
        "*  Story IDS of the work\n",
        "*  Updated date of the work\n",
        "*  Title and author of the work\n",
        "*  Language of the work\n",
        "*  Number of words in the work\n",
        "*  Number of chapter in the work\n",
        "*  Number of hits \n",
        "*  Requiered tags \n",
        "*  Additionnal tags"
      ],
      "metadata": {
        "id": "4xcOY-y3OGH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code do the same as Web Srapping AO3 part 1 but only to append the database.\n",
        "\n",
        "It was supposed to be quicker than part 1 but it's not and I don't really get why.\n",
        "\n",
        "EDIT : So actually this code is indeed faster when you append more than 50 page, I tried to append 100 page and this code (part 2) took 368 seconds and the other (part 1) 395 seconds to run (without the graph and histogram etc)."
      ],
      "metadata": {
        "id": "x4lDbHG4tJnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# ⚠ WARNING ⚠\n",
        "Two important thing to understand when you download **many files** of data :\n",
        "\n",
        "\n",
        "1.   **You can't append more than 70 pages in one time**\n",
        "\n",
        "  If you want to append a lot of data I suggest 50 page by 50 page. I mean if you want to try to append more that's your choice but the code is optimized to append 50 by 50 (or less).\n",
        "\n",
        "  Also, one page is 20 work, so each time you run the code you can append at max 1400 works.\n",
        "\n",
        "2.   **You need to have the SAME interval of page between files**\n",
        "\n",
        "  Per example, you want to append the work from a ertain fandom, you have 457 pages in total. I would suggest to append page 1 to 50 (base_page_0_to_51), then 50 to 100 (base_page_50_to_101), ... , then 400 to 450 (base_page_400_to_450 and finally page 450 to 456 (base_page_450_to_457).\n",
        "\n",
        "  For the last one you input 456 as Number_page but its append untill page 457 because my code is WEIRD.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tKY3x79TPA8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section doesn't have a lot of comments, if you're curious on how the code work I suggest you to check part 1 (it's the same result at the end)"
      ],
      "metadata": {
        "id": "CiCAvJtAaBX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Library**"
      ],
      "metadata": {
        "id": "6mhpDSaRtr-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U1fS6u2GiiNR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import csv\n",
        "from time import sleep\n",
        "import pickle\n",
        "from os import path\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import urlparse\n",
        "from google.colab import files\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**"
      ],
      "metadata": {
        "id": "6oI06n8Btvhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as part 1, we ask you to input an URL, a first page and a last one."
      ],
      "metadata": {
        "id": "GyfDsHUDtyIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the fandom, the beginning of the page and the ending of the page of the data you want to append\n",
        "\n",
        "# ULR\n",
        "TEST_URL = \"https://archiveofourown.org/tags/Stranger%20Things%20(TV%202016)/\"\n",
        "\n",
        "# First_Page\n",
        "First_Page = 1\n",
        "Beginning_Page = First_Page\n",
        "\n",
        "# Last_page\n",
        "Last_page = 100\n",
        "Last_page = Last_page + 1\n",
        "Number_page = Last_page"
      ],
      "metadata": {
        "id": "Nzvzrj3ujBOP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the tag\n",
        "tag= urlparse(TEST_URL)\n",
        "tag = tag.path\n",
        "tag_2 = tag\n",
        "tag_2 = tag_2.replace('tags','')\n",
        "tag_2 = tag_2.replace('/','')\n",
        "tag_2 = tag_2.replace('%20',' ')\n",
        "tag_2 = tag_2.replace('*s*',' and ')"
      ],
      "metadata": {
        "id": "OFKRxxWxjTHP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Download Data for the following tag : \",tag_2,\"from page\",First_Page,\"to\",Last_page)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmg4CC7RjpQL",
        "outputId": "497601de-b65c-4b7d-ac89-74d12cd7fb9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download Data for the following tag :  Stranger Things (TV 2016) from page 1 to 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define functions**"
      ],
      "metadata": {
        "id": "YLHpl31VmwdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and soupify"
      ],
      "metadata": {
        "id": "ZFVwLPvbuebW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_soupify(url, parser=\"html.parser\"):\n",
        "\t\"\"\"\n",
        "\tGiven a URL, downloads the site and turns it into beautiful soup.\n",
        "\t\"\"\"\n",
        "\n",
        "\tfull_url = url + \"works\"\n",
        "\tresponse = urllib.request.urlopen(full_url)\n",
        "\tdirectory_html = response.read()\n",
        "\tindex_soup = BeautifulSoup(directory_html, parser)\n",
        "\n",
        "\treturn index_soup"
      ],
      "metadata": {
        "id": "ToSOUt9Qmxy1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get directory urls"
      ],
      "metadata": {
        "id": "jr1irwt8umrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_directory_urls(url, index_soup):\n",
        "\t\"\"\"\n",
        "\tGiven the base directory as soup, figure out how many pages there are,\n",
        "\tand return a list of the URLs of each directory page.\n",
        "\t\"\"\"\n",
        "\n",
        "\tpage_numbers = index_soup.find_all(\"ol\", class_=\"pagination actions\")\n",
        "\n",
        "\tli_entries = page_numbers[0].find_all(\"li\")\n",
        "\n",
        "\tli_texts = [number.text for number in li_entries]\n",
        "\tli_digits = [int(number) for number in li_texts if number.isdigit()]\n",
        "\tsorted_page_numbers = sorted(li_digits)\n",
        "\n",
        "\tnumber_of_pages = sorted_page_numbers[-1]\n",
        "\n",
        "\turl_prefix = url + \"works?page=\"\n",
        "\tdirectory_urls = [(url_prefix + str(page_number)) for page_number in range(0, number_of_pages+1)]\n",
        "\n",
        "\treturn directory_urls"
      ],
      "metadata": {
        "id": "-73_Y0T_j-Tm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Story IDS"
      ],
      "metadata": {
        "id": "AAISABoiuomG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_story_ids(directory_url):\n",
        "\t\"\"\"\n",
        "\tGiven a single directory URL, get all stories from that URL. Returns a list of story IDs.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdir_soup = download_and_soupify(directory_url)\n",
        "\tdir_links = dir_soup.find_all(\"a\", href=True)\n",
        "\n",
        "\tdir_hrefs = [link.attrs['href'] for link in dir_links]\n",
        "\twork_hrefs = [link for link in dir_hrefs if \"/works/\" in link]\n",
        "\tpotential_ids = [work.split(\"/\")[2] for work in work_hrefs]\n",
        "\tid_list = [int(work_id) for work_id in potential_ids if work_id.isdigit()]\n",
        "\n",
        "\tstory_ids = list(set(id_list))\n",
        "\n",
        "\treturn story_ids"
      ],
      "metadata": {
        "id": "DKe0M2GUlCDo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date update of the work"
      ],
      "metadata": {
        "id": "bcbtne8iurw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datetime(directory_url):\n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"p\",{\"class\":\"datetime\"}):\n",
        "    text=x.get_text()\n",
        "    date_work.append(str(text))"
      ],
      "metadata": {
        "id": "-OIiZcoFlEyC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title and author of the work"
      ],
      "metadata": {
        "id": "hikkzg2Xuy4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def worktitle(directory_url):\n",
        "  \n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"h4\",attrs={\"class\": \"heading\"}):  \n",
        "    text=x.get_text()\n",
        "    work_title_authors.append(str(text))\n",
        "\n",
        "  work_title_authors.remove('\\n            Include\\n          ')\n",
        "  work_title_authors.remove('\\n            Exclude\\n          ')\n",
        "  work_title_authors.remove('More Options')\n",
        "  work_title_authors.remove('Pages Navigation')\n",
        "  work_title_authors.remove('About the Archive')\n",
        "  work_title_authors.remove('Pages Navigation')\n",
        "  work_title_authors.remove('Contact Us')\n",
        "  work_title_authors.remove('Development')\n",
        "\n",
        "  (work_title_authors)\n",
        "\n",
        "# It quit the same as the other function \n",
        "# Exept this time we need to remove a lot of stuff by page to clean the text\n"
      ],
      "metadata": {
        "id": "epc0deudlLXN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language of the work"
      ],
      "metadata": {
        "id": "q-w13ppfu0Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def worklanguage(directory_url):\n",
        "  \n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"dd\",attrs={\"class\": \"language\"}):  \n",
        "    text=x.get_text()\n",
        "    work_language.append(str(text))\n",
        "\n",
        "  (work_language)\n",
        "\n",
        "# I guess you know how it works now"
      ],
      "metadata": {
        "id": "6VYPjZ2-lN7f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of words of the work"
      ],
      "metadata": {
        "id": "rsjiyCVHu2Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NumberofWords(directory_url):\n",
        "  \n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"dd\",attrs={\"class\": \"words\"}):  \n",
        "    text=x.get_text()\n",
        "    Nbr_words.append(str(text))"
      ],
      "metadata": {
        "id": "3JRbuSeFlRl0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Chapter"
      ],
      "metadata": {
        "id": "uJLgVWCHu4QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NumberofChapter(directory_url):\n",
        "  \n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"dd\",attrs={\"class\": \"chapters\"}):  \n",
        "    text=x.get_text()\n",
        "    Nbr_chapter.append(str(text))"
      ],
      "metadata": {
        "id": "Tg7hKWOvlTMI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Hits"
      ],
      "metadata": {
        "id": "iL65Zjg9u7vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NumberofHits(directory_url):\n",
        "  \n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"dd\",attrs={\"class\": \"hits\"}):  \n",
        "    text=x.get_text()\n",
        "    Nbr_hits.append(str(text))"
      ],
      "metadata": {
        "id": "gk2zfbDzlUTY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requiered tags"
      ],
      "metadata": {
        "id": "Jt-umwdQu9_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tags_req(directory_url):\n",
        "  \n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"ul\",attrs={\"class\": \"required-tags\"}):  \n",
        "    text=x.get_text()\n",
        "    tags_requiered.append(str(text))"
      ],
      "metadata": {
        "id": "BZKeYeIqDdVo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionals tags"
      ],
      "metadata": {
        "id": "xY0kqGX3vAXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tags(directory_url):\n",
        "  \n",
        "  dir_soup = download_and_soupify(directory_url)\n",
        "  for x in dir_soup.findAll(\"ul\",attrs={\"class\": \"tags commas\"}):  \n",
        "    text=x.get_text()\n",
        "    tags_add.append(str(text))"
      ],
      "metadata": {
        "id": "3T_DYkqzlhN7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Big function for everything**"
      ],
      "metadata": {
        "id": "rHDXQD9Qlo61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_soup = download_and_soupify(TEST_URL)"
      ],
      "metadata": {
        "id": "VqVc8P4Mm0Oc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_urls = get_directory_urls(TEST_URL,index_soup)"
      ],
      "metadata": {
        "id": "y6eV9tbBloPy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty storage\n",
        "story_ids = []\n",
        "story_ids_2 = []\n",
        "\n",
        "date_work = []\n",
        "\n",
        "work_title_authors = []\n",
        "\n",
        "work_language = []\n",
        "work_language_2 = []\n",
        "\n",
        "Nbr_words = []\n",
        "\n",
        "Nbr_chapter = []\n",
        "\n",
        "Nbr_hits = []\n",
        "\n",
        "tags_requiered = []\n",
        "\n",
        "tags_add = []\n",
        "\n",
        "\n",
        "index_page = []"
      ],
      "metadata": {
        "id": "fBIwLEfMrhh3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_append(URL, Beginning_Page, Number_page):\n",
        "  \n",
        "  for i in range(Beginning_Page,Number_page):\n",
        "    # story_ids_2\n",
        "    story_ids.append(get_story_ids(directory_urls[i]))\n",
        "\n",
        "    # data_updated\n",
        "    date_work.append(datetime(directory_urls[i]))\n",
        "    date_work.remove(None)\n",
        "\n",
        "    # work_title_authors\n",
        "    work_title_authors.append(worktitle(directory_urls[i]))\n",
        "    work_title_authors.remove(None)\n",
        "\n",
        "    # work_language\n",
        "    work_language.append(worklanguage(directory_urls[i]))\n",
        "    work_language.remove(None)\n",
        "\n",
        "    # Number of words\n",
        "    Nbr_words.append(NumberofWords(directory_urls[i]))\n",
        "    Nbr_words.remove(None)\n",
        "\n",
        "    # Number of chapter\n",
        "    Nbr_chapter.append(NumberofChapter(directory_urls[i]))\n",
        "    Nbr_chapter.remove(None)\n",
        "\n",
        "    # Number of hits\n",
        "    Nbr_hits.append(NumberofHits(directory_urls[i]))\n",
        "    Nbr_hits.remove(None)\n",
        "\n",
        "    # Requiered tags\n",
        "    tags_requiered.append(tags_req(directory_urls[i]))\n",
        "    tags_requiered.remove(None)\n",
        "\n",
        "    # Additionnal tags\n",
        "    tags_add.append(tags(directory_urls[i]))\n",
        "    tags_add.remove(None)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "    # index_page\n",
        "    for j in range(20):\n",
        "      index_page.append(i)\n",
        "\n",
        "  # work_title_authors\n",
        "  for work in range(len(work_title_authors)):\n",
        "    work_title_authors[work] = work_title_authors[work].replace('\\n',' ')\n",
        "    work_title_authors[work] = work_title_authors[work].replace('       ',' ')\n",
        "    work_title_authors[work] = work_title_authors[work].replace('     ',' ')\n",
        "\n",
        "  # work_language\n",
        "  Number_page_bis = Number_page - Beginning_Page\n",
        "  Number_page_bis = Number_page_bis\n",
        "  Drop_language_setting = []\n",
        "  for i in range(Number_page_bis):\n",
        "    Drop_language_setting_bis = 20\n",
        "    Drop_language_setting_bis = Drop_language_setting_bis + i*20\n",
        "    Drop_language_setting.append(Drop_language_setting_bis)\n",
        "  Drop_language_setting\n",
        "\n",
        "  for i in Drop_language_setting:\n",
        "    work_language.remove(work_language[i])\n",
        "  \n",
        "  # Requiered tags and additionnal tags\n",
        "  for i in range(len(tags_requiered)):\n",
        "    tags_requiered[i] = tags_requiered[i].replace(\"\\n\",\"|\")\n",
        "    tags_add[i] = tags_add[i].replace(\"\\n\",\"|\")\n",
        "    backslash = '\"\\\\\"'\n",
        "    tags_add[i] = tags_add[i].replace(backslash,'')"
      ],
      "metadata": {
        "id": "KOMNDlTul3Mw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the function**"
      ],
      "metadata": {
        "id": "mtRr_GTUvgEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_append(TEST_URL,First_Page,Last_page)"
      ],
      "metadata": {
        "id": "YLyA_bZnnUO8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,len(story_ids)):\n",
        " story_ids_2 = story_ids_2 + story_ids[i]\n",
        "\n",
        "# Adjust on story_ids "
      ],
      "metadata": {
        "id": "shRbBPfa-6au"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inside a DataFrame**"
      ],
      "metadata": {
        "id": "X3ohxaQ-FC_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe\n",
        "base1 = {'Index_page' : index_page, \n",
        "          'ID' :story_ids_2,\n",
        "          'Date_publication' : date_work, \n",
        "          'Title_and_author' : work_title_authors, \n",
        "          'Language_of_the_work' : work_language, \n",
        "          'Word' : Nbr_words,\n",
        "          'Chapter' : Nbr_chapter\n",
        "          , 'Hits' : Nbr_hits\n",
        "          , 'Requiered_tags' : tags_requiered\n",
        "          , 'Additionnal_tags' : tags_add}\n",
        "base1 = pd.DataFrame(data=base1)\n",
        "base1"
      ],
      "metadata": {
        "id": "geY4rM69yHM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "66f04ea1-6a64-40b8-d84e-8961de236c27"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Index_page        ID Date_publication  \\\n",
              "0              1  44157319      02 Feb 2023   \n",
              "1              1  44733832      02 Feb 2023   \n",
              "2              1  21311245      02 Feb 2023   \n",
              "3              1  44511886      02 Feb 2023   \n",
              "4              1  43212331      02 Feb 2023   \n",
              "...          ...       ...              ...   \n",
              "1995         100  44318299      22 Jan 2023   \n",
              "1996         100  40882788      22 Jan 2023   \n",
              "1997         100  44463469      22 Jan 2023   \n",
              "1998         100  44462842      22 Jan 2023   \n",
              "1999         100  40308990      22 Jan 2023   \n",
              "\n",
              "                                       Title_and_author Language_of_the_work  \\\n",
              "0                    Harringrove Stuff by wrecked_fuse               English   \n",
              "1      The Past, The Future and The Present by Sodap...              English   \n",
              "2      Like Magnets by Pondermoniums  for wrecked_fuse               English   \n",
              "3                   Be Patient by TATAMONIQUELAGINETTE               English   \n",
              "4      Midnight (Belongs to You and Me) by Polkadotd...              English   \n",
              "...                                                 ...                  ...   \n",
              "1995   something happens and i'm head over heels by ...              English   \n",
              "1996                Love of a Lifetime by dibidibidani               English   \n",
              "1997                  Home by the Sea by WreakingHavok               English   \n",
              "1998            Asking You to Love Me by chattrekisses               English   \n",
              "1999   hum along 'til the feelings gone, forever by ...              English   \n",
              "\n",
              "        Word Chapter    Hits  \\\n",
              "0          0   60/60  111004   \n",
              "1      2,278     1/2       0   \n",
              "2     25,522    18/?    7588   \n",
              "3      2,395     1/1       0   \n",
              "4     24,448     4/?    1254   \n",
              "...      ...     ...     ...   \n",
              "1995  12,694     5/?    1063   \n",
              "1996  19,053     7/?     667   \n",
              "1997  11,716     4/5    1965   \n",
              "1998  22,397     1/1    1999   \n",
              "1999  22,012     4/4    2758   \n",
              "\n",
              "                                         Requiered_tags  \\\n",
              "0     | Explicit| Choose Not To Use Archive Warnings...   \n",
              "1     | Explicit| Underage| M/M, Multi| Work in Prog...   \n",
              "2     | Teen And Up Audiences| No Archive Warnings A...   \n",
              "3     | Explicit| No Archive Warnings Apply| F/F| Co...   \n",
              "4     | Explicit| Choose Not To Use Archive Warnings...   \n",
              "...                                                 ...   \n",
              "1995  | Explicit| Choose Not To Use Archive Warnings...   \n",
              "1996  | Teen And Up Audiences| No Archive Warnings A...   \n",
              "1997  | General Audiences| Choose Not To Use Archive...   \n",
              "1998  | Explicit| No Archive Warnings Apply| M/M| Co...   \n",
              "1999  | Explicit| Choose Not To Use Archive Warnings...   \n",
              "\n",
              "                                       Additionnal_tags  \n",
              "0     |Creator Chose Not To Use Archive Warnings No ...  \n",
              "1     |UnderageWill Byers/Mike Wheeler Mike Wheeler/...  \n",
              "2     |No Archive Warnings ApplyBilly Hargrove/Steve...  \n",
              "3     |No Archive Warnings ApplyRobin Buckley/Nancy ...  \n",
              "4     |Creator Chose Not To Use Archive WarningsChri...  \n",
              "...                                                 ...  \n",
              "1995  |Creator Chose Not To Use Archive WarningsStev...  \n",
              "1996  |No Archive Warnings ApplyChrissy Cunningham/E...  \n",
              "1997  |Creator Chose Not To Use Archive WarningsWill...  \n",
              "1998  |No Archive Warnings ApplySteve Harrington/Edd...  \n",
              "1999  |Creator Chose Not To Use Archive WarningsStev...  \n",
              "\n",
              "[2000 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-682defe6-1326-4746-8385-a594d2b305ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index_page</th>\n",
              "      <th>ID</th>\n",
              "      <th>Date_publication</th>\n",
              "      <th>Title_and_author</th>\n",
              "      <th>Language_of_the_work</th>\n",
              "      <th>Word</th>\n",
              "      <th>Chapter</th>\n",
              "      <th>Hits</th>\n",
              "      <th>Requiered_tags</th>\n",
              "      <th>Additionnal_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>44157319</td>\n",
              "      <td>02 Feb 2023</td>\n",
              "      <td>Harringrove Stuff by wrecked_fuse</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "      <td>60/60</td>\n",
              "      <td>111004</td>\n",
              "      <td>| Explicit| Choose Not To Use Archive Warnings...</td>\n",
              "      <td>|Creator Chose Not To Use Archive Warnings No ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>44733832</td>\n",
              "      <td>02 Feb 2023</td>\n",
              "      <td>The Past, The Future and The Present by Sodap...</td>\n",
              "      <td>English</td>\n",
              "      <td>2,278</td>\n",
              "      <td>1/2</td>\n",
              "      <td>0</td>\n",
              "      <td>| Explicit| Underage| M/M, Multi| Work in Prog...</td>\n",
              "      <td>|UnderageWill Byers/Mike Wheeler Mike Wheeler/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>21311245</td>\n",
              "      <td>02 Feb 2023</td>\n",
              "      <td>Like Magnets by Pondermoniums  for wrecked_fuse</td>\n",
              "      <td>English</td>\n",
              "      <td>25,522</td>\n",
              "      <td>18/?</td>\n",
              "      <td>7588</td>\n",
              "      <td>| Teen And Up Audiences| No Archive Warnings A...</td>\n",
              "      <td>|No Archive Warnings ApplyBilly Hargrove/Steve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>44511886</td>\n",
              "      <td>02 Feb 2023</td>\n",
              "      <td>Be Patient by TATAMONIQUELAGINETTE</td>\n",
              "      <td>English</td>\n",
              "      <td>2,395</td>\n",
              "      <td>1/1</td>\n",
              "      <td>0</td>\n",
              "      <td>| Explicit| No Archive Warnings Apply| F/F| Co...</td>\n",
              "      <td>|No Archive Warnings ApplyRobin Buckley/Nancy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>43212331</td>\n",
              "      <td>02 Feb 2023</td>\n",
              "      <td>Midnight (Belongs to You and Me) by Polkadotd...</td>\n",
              "      <td>English</td>\n",
              "      <td>24,448</td>\n",
              "      <td>4/?</td>\n",
              "      <td>1254</td>\n",
              "      <td>| Explicit| Choose Not To Use Archive Warnings...</td>\n",
              "      <td>|Creator Chose Not To Use Archive WarningsChri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>100</td>\n",
              "      <td>44318299</td>\n",
              "      <td>22 Jan 2023</td>\n",
              "      <td>something happens and i'm head over heels by ...</td>\n",
              "      <td>English</td>\n",
              "      <td>12,694</td>\n",
              "      <td>5/?</td>\n",
              "      <td>1063</td>\n",
              "      <td>| Explicit| Choose Not To Use Archive Warnings...</td>\n",
              "      <td>|Creator Chose Not To Use Archive WarningsStev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>100</td>\n",
              "      <td>40882788</td>\n",
              "      <td>22 Jan 2023</td>\n",
              "      <td>Love of a Lifetime by dibidibidani</td>\n",
              "      <td>English</td>\n",
              "      <td>19,053</td>\n",
              "      <td>7/?</td>\n",
              "      <td>667</td>\n",
              "      <td>| Teen And Up Audiences| No Archive Warnings A...</td>\n",
              "      <td>|No Archive Warnings ApplyChrissy Cunningham/E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>100</td>\n",
              "      <td>44463469</td>\n",
              "      <td>22 Jan 2023</td>\n",
              "      <td>Home by the Sea by WreakingHavok</td>\n",
              "      <td>English</td>\n",
              "      <td>11,716</td>\n",
              "      <td>4/5</td>\n",
              "      <td>1965</td>\n",
              "      <td>| General Audiences| Choose Not To Use Archive...</td>\n",
              "      <td>|Creator Chose Not To Use Archive WarningsWill...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>100</td>\n",
              "      <td>44462842</td>\n",
              "      <td>22 Jan 2023</td>\n",
              "      <td>Asking You to Love Me by chattrekisses</td>\n",
              "      <td>English</td>\n",
              "      <td>22,397</td>\n",
              "      <td>1/1</td>\n",
              "      <td>1999</td>\n",
              "      <td>| Explicit| No Archive Warnings Apply| M/M| Co...</td>\n",
              "      <td>|No Archive Warnings ApplySteve Harrington/Edd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>100</td>\n",
              "      <td>40308990</td>\n",
              "      <td>22 Jan 2023</td>\n",
              "      <td>hum along 'til the feelings gone, forever by ...</td>\n",
              "      <td>English</td>\n",
              "      <td>22,012</td>\n",
              "      <td>4/4</td>\n",
              "      <td>2758</td>\n",
              "      <td>| Explicit| Choose Not To Use Archive Warnings...</td>\n",
              "      <td>|Creator Chose Not To Use Archive WarningsStev...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-682defe6-1326-4746-8385-a594d2b305ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-682defe6-1326-4746-8385-a594d2b305ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-682defe6-1326-4746-8385-a594d2b305ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "uJI7D73FEnZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a650a81d-e85b-4584-b6d4-9d9f068d71d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 368.374635219574 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export the DataBase**"
      ],
      "metadata": {
        "id": "biOECddoGNNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download your database on your computer"
      ],
      "metadata": {
        "id": "vU5Cry7_aeiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_export = base1\n",
        "Name_base = \"base_page_\" + str(Beginning_Page) + \"_to_\" + str(Number_page) + \".csv\"\n",
        "Name_base"
      ],
      "metadata": {
        "id": "wpm5m9tYE2v_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f3637f87-e1f1-4a25-ab36-c0297f03f9fa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'base_page_1_to_101.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from google.colab import files\n",
        "base_export.to_csv(Name_base) \n",
        "files.download(Name_base)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WLQtJHmDI2Fw",
        "outputId": "d87a6970-591b-49e4-fb19-da3f24e3cd40"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom google.colab import files\\nbase_export.to_csv(Name_base) \\nfiles.download(Name_base)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}